"""
Legal Data Collector - Sistema de coleta automatizada de dados jurÃ­dicos brasileiros
Coleta leis, jurisprudÃªncia e regulamentaÃ§Ãµes de fontes oficiais
"""
import asyncio
import aiohttp
import aiofiles
import re
import json
from typing import Dict, List, Optional, Tuple
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
from datetime import datetime
import hashlib
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class LegalDataCollector:
    """Coletor automatizado e Ã©tico de dados jurÃ­dicos brasileiros"""
    
    def __init__(self):
        self.session = None
        self.rate_limit_delay = 3.0  # 3 segundos entre requests para ser respeitoso
        self.collected_data = []
        
        # Headers para parecer um navegador real e ser respeitoso
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'pt-BR,pt;q=0.8,en;q=0.5',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        }
    
    async def __aenter__(self):
        """Context manager para gerenciar sessÃ£o HTTP"""
        connector = aiohttp.TCPConnector(limit=1, limit_per_host=1)
        timeout = aiohttp.ClientTimeout(total=30, connect=10)
        self.session = aiohttp.ClientSession(
            connector=connector,
            timeout=timeout,
            headers=self.headers
        )
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """Fechar sessÃ£o HTTP"""
        if self.session:
            await self.session.close()
    
    async def _fetch_with_rate_limit(self, url: str) -> Optional[str]:
        """Fetch URL com rate limiting e tratamento de erros"""
        
        try:
            logger.info(f"ğŸŒ Coletando: {url}")
            
            # Rate limiting respeitoso
            await asyncio.sleep(self.rate_limit_delay)
            
            async with self.session.get(url) as response:
                if response.status == 200:
                    content = await response.text(encoding='utf-8')
                    logger.info(f"âœ… Coletado com sucesso: {len(content)} caracteres")
                    return content
                else:
                    logger.warning(f"âš ï¸  Status {response.status} para {url}")
                    return None
                    
        except asyncio.TimeoutError:
            logger.error(f"â° Timeout para {url}")
            return None
        except Exception as e:
            logger.error(f"âŒ Erro ao coletar {url}: {str(e)}")
            return None
    
    async def collect_planalto_laws(self) -> List[Dict]:
        """Coleta leis fundamentais do portal planalto.gov.br"""
        
        logger.info("ğŸ“š Iniciando coleta de leis do Planalto...")
        
        # Leis fundamentais para anÃ¡lise de contratos
        fundamental_laws = {
            "CÃ³digo Civil - Contratos": {
                "url": "http://www.planalto.gov.br/ccivil_03/leis/2002/l10406.htm",
                "category": "geral",
                "priority_sections": ["TÃTULO V", "CAPÃTULO I", "SeÃ§Ã£o I"]  # SeÃ§Ãµes sobre contratos
            },
            "CÃ³digo de Defesa do Consumidor": {
                "url": "http://www.planalto.gov.br/ccivil_03/leis/l8078.htm", 
                "category": "geral",
                "priority_sections": ["CAPÃTULO VI"]  # PrÃ¡ticas abusivas
            },
            "Lei do Inquilinato": {
                "url": "http://www.planalto.gov.br/ccivil_03/leis/l8245.htm",
                "category": "locacao",
                "priority_sections": ["CAPÃTULO I", "CAPÃTULO II"]
            },
            "Lei Geral de TelecomunicaÃ§Ãµes": {
                "url": "http://www.planalto.gov.br/ccivil_03/leis/l9472.htm",
                "category": "telecom", 
                "priority_sections": ["TÃTULO V"]  # Direitos dos usuÃ¡rios
            },
            "Lei do Sistema Financeiro Nacional": {
                "url": "http://www.planalto.gov.br/ccivil_03/leis/l4595.htm",
                "category": "financeiro",
                "priority_sections": ["CAPÃTULO III"]
            }
        }
        
        laws_data = []
        
        for law_name, law_info in fundamental_laws.items():
            try:
                html_content = await self._fetch_with_rate_limit(law_info["url"])
                
                if html_content:
                    # Parse e limpeza do HTML
                    cleaned_content = self._parse_planalto_law(html_content, law_name)
                    
                    if cleaned_content:
                        # Criar hash Ãºnico para detectar mudanÃ§as futuras
                        content_hash = hashlib.md5(cleaned_content.encode()).hexdigest()
                        
                        law_data = {
                            "title": law_name,
                            "content": cleaned_content,
                            "document_type": "lei",
                            "category": law_info["category"],
                            "source": "Planalto.gov.br",
                            "source_url": law_info["url"],
                            "reference_number": self._extract_law_number(law_name),
                            "authority_level": "high",
                            "legal_area": self._determine_legal_areas(law_name),
                            "keywords": self._extract_keywords(law_name, cleaned_content),
                            "collection_date": datetime.now().isoformat(),
                            "content_hash": content_hash
                        }
                        
                        laws_data.append(law_data)
                        logger.info(f"âœ… Lei processada: {law_name}")
                    else:
                        logger.warning(f"âš ï¸  ConteÃºdo vazio para {law_name}")
                else:
                    logger.error(f"âŒ Falha ao coletar {law_name}")
                    
            except Exception as e:
                logger.error(f"âŒ Erro processando {law_name}: {str(e)}")
                continue
        
        logger.info(f"ğŸ“‹ Total de leis coletadas: {len(laws_data)}")
        return laws_data
    
    def _parse_planalto_law(self, html_content: str, law_name: str) -> str:
        """Parse especÃ­fico para estrutura HTML do Planalto"""
        
        soup = BeautifulSoup(html_content, 'html.parser')
        
        # Remover elementos desnecessÃ¡rios
        for tag in soup(["script", "style", "nav", "header", "footer"]):
            tag.decompose()
        
        # Tentar diferentes seletores para o conteÃºdo principal
        content_selectors = [
            'div[align="justify"]',  # Estrutura comum do Planalto
            'div.texto-lei',
            '#content-core',
            'body > div:nth-child(3)',  # Estrutura especÃ­fica observada
            'td[align="justify"]'  # Tabelas com texto da lei
        ]
        
        text_content = ""
        
        for selector in content_selectors:
            elements = soup.select(selector)
            if elements:
                for element in elements:
                    text = element.get_text(separator='\\n', strip=True)
                    if len(text) > 1000:  # Filtro por tamanho mÃ­nimo
                        text_content += text + "\\n\\n"
                break
        
        # Se nÃ£o encontrou pelos seletores, pegar todo o body
        if not text_content:
            body = soup.find('body')
            if body:
                text_content = body.get_text(separator='\\n', strip=True)
        
        # Limpeza e normalizaÃ§Ã£o do texto
        cleaned_text = self._clean_legal_text(text_content)
        
        # Filtrar apenas o texto da lei (remover navegaÃ§Ã£o, etc.)
        law_text = self._extract_law_content(cleaned_text, law_name)
        
        return law_text
    
    def _clean_legal_text(self, text: str) -> str:
        """Limpa e normaliza texto jurÃ­dico extraÃ­do"""
        
        # Remover elementos de navegaÃ§Ã£o e layout
        navigation_patterns = [
            r"PresidÃªncia da RepÃºblica.*?Casa Civil",
            r"Subchefia para Assuntos JurÃ­dicos",
            r"Imprimir.*?Voltar ao topo", 
            r"Este texto nÃ£o substitui.*?publicado",
            r"Vide.*?Regulamento",
            r"\\(Vide.*?\\)",
            r"Menu.*?Buscar",
            r"Compartilhar.*?Imprimir"
        ]
        
        cleaned = text
        for pattern in navigation_patterns:
            cleaned = re.sub(pattern, "", cleaned, flags=re.IGNORECASE | re.DOTALL)
        
        # Normalizar espaÃ§os em branco
        cleaned = re.sub(r'\\s+', ' ', cleaned)
        cleaned = re.sub(r'\\n\\s*\\n', '\\n\\n', cleaned)  # Duplas quebras
        
        # Remover linhas muito curtas (provavelmente navegaÃ§Ã£o)
        lines = cleaned.split('\\n')
        meaningful_lines = []
        
        for line in lines:
            line = line.strip()
            if len(line) > 10 and not self._is_navigation_line(line):
                meaningful_lines.append(line)
        
        return '\\n'.join(meaningful_lines)
    
    def _is_navigation_line(self, line: str) -> bool:
        """Identifica se uma linha Ã© elemento de navegaÃ§Ã£o"""
        
        navigation_keywords = [
            "menu", "buscar", "imprimir", "voltar", "topo",
            "compartilhar", "facebook", "twitter", "whatsapp",
            "presidÃªncia", "casa civil", "planalto"
        ]
        
        line_lower = line.lower()
        return any(keyword in line_lower for keyword in navigation_keywords)
    
    def _extract_law_content(self, text: str, law_name: str) -> str:
        """Extrai apenas o conteÃºdo da lei, removendo preÃ¢mbulos"""
        
        # Procurar pelo inÃ­cio tÃ­pico de leis brasileiras
        law_start_patterns = [
            r"Art\\.?\\s*1[Â°Âº]",  # Artigo 1Â°
            r"Artigo\\s+1[Â°Âº]",
            r"CAPÃTULO\\s+I",
            r"TÃTULO\\s+I",
            law_name.split()[0] if law_name else ""
        ]
        
        for pattern in law_start_patterns:
            match = re.search(pattern, text, re.IGNORECASE)
            if match:
                # Pegar do inÃ­cio da lei em diante
                law_content = text[match.start():]
                
                # Remover rodapÃ© se houver
                footer_patterns = [
                    r"BrasÃ­lia.*?\\d{4}",
                    r"Este texto.*?DOU",
                    r"Publicado.*?SeÃ§Ã£o"
                ]
                
                for footer_pattern in footer_patterns:
                    law_content = re.sub(footer_pattern, "", law_content, flags=re.IGNORECASE | re.DOTALL)
                
                return law_content.strip()
        
        # Se nÃ£o achou padrÃ£o especÃ­fico, retornar texto limpo
        return text
    
    def _extract_law_number(self, law_name: str) -> str:
        """Extrai nÃºmero/referÃªncia da lei"""
        
        # Procurar padrÃµes como "Lei 8.245/91", "Decreto 1.234/2020"
        patterns = [
            r"Lei\\s+(\\d+\\.?\\d*/\\d+)",
            r"Decreto\\s+(\\d+\\.?\\d*/\\d+)",
            r"CÃ³digo\\s+(\\w+)"
        ]
        
        for pattern in patterns:
            match = re.search(pattern, law_name, re.IGNORECASE)
            if match:
                return match.group(1)
        
        return law_name
    
    def _determine_legal_areas(self, law_name: str) -> List[str]:
        """Determina Ã¡reas do direito aplicÃ¡veis"""
        
        area_mapping = {
            "civil": ["direito civil", "contratos"],
            "consumidor": ["direito do consumidor", "contratos"],
            "inquilinato": ["direito imobiliÃ¡rio", "locaÃ§Ã£o"],
            "telecomunicaÃ§Ãµes": ["direito das telecomunicaÃ§Ãµes", "regulatÃ³rio"],
            "financeiro": ["direito bancÃ¡rio", "financeiro"]
        }
        
        law_lower = law_name.lower()
        areas = []
        
        for key, values in area_mapping.items():
            if key in law_lower:
                areas.extend(values)
        
        return areas if areas else ["direito geral"]
    
    def _extract_keywords(self, law_name: str, content: str) -> List[str]:
        """Extrai palavras-chave relevantes"""
        
        # Keywords especÃ­ficas por tipo de lei
        base_keywords = []
        
        if "civil" in law_name.lower():
            base_keywords = ["contrato", "obrigaÃ§Ã£o", "direito", "clÃ¡usula"]
        elif "consumidor" in law_name.lower():
            base_keywords = ["consumidor", "fornecedor", "abusiva", "direito"]
        elif "inquilinato" in law_name.lower():
            base_keywords = ["locaÃ§Ã£o", "aluguel", "locador", "locatÃ¡rio"]
        elif "telecomunicaÃ§Ãµes" in law_name.lower():
            base_keywords = ["telecomunicaÃ§Ãµes", "usuÃ¡rio", "serviÃ§o", "ANATEL"]
        
        # Extrair termos jurÃ­dicos do conteÃºdo
        legal_terms_pattern = r"\\b(artigo?|parÃ¡grafo|inciso|alÃ­nea|clÃ¡usula|contrato|direito|dever|obrigaÃ§Ã£o)\\b"
        legal_terms = re.findall(legal_terms_pattern, content.lower())
        
        # Combinar e deduplicar
        all_keywords = base_keywords + list(set(legal_terms))
        
        return list(set(all_keywords))[:10]  # Limitar a 10 keywords
    
    async def collect_stj_jurisprudence(self, search_terms: List[str]) -> List[Dict]:
        """Coleta jurisprudÃªncia do STJ"""
        
        logger.info("âš–ï¸ Iniciando coleta de jurisprudÃªncia STJ...")
        
        jurisprudence_data = []
        
        # URLs base do STJ
        stj_search_base = "https://scon.stj.jus.br/SCON/pesquisar.jsp"
        
        for term in search_terms:
            try:
                logger.info(f"ğŸ” Buscando jurisprudÃªncia para: {term}")
                
                # Construir URL de busca do STJ
                search_url = f"{stj_search_base}?pesq={term.replace(' ', '+')}&tipo_visualizacao=RESUMO"
                
                html_content = await self._fetch_with_rate_limit(search_url)
                
                if html_content:
                    precedents = self._parse_stj_results(html_content, term)
                    jurisprudence_data.extend(precedents)
                    
                    logger.info(f"âœ… Encontrados {len(precedents)} precedentes para '{term}'")
                
            except Exception as e:
                logger.error(f"âŒ Erro ao buscar jurisprudÃªncia para '{term}': {str(e)}")
                continue
        
        logger.info(f"ğŸ“‹ Total de jurisprudÃªncia coletada: {len(jurisprudence_data)}")
        return jurisprudence_data
    
    def _parse_stj_results(self, html_content: str, search_term: str) -> List[Dict]:
        """Parse dos resultados de busca do STJ"""
        
        soup = BeautifulSoup(html_content, 'html.parser')
        precedents = []
        
        # Procurar por elementos tÃ­picos de resultado do STJ
        result_selectors = [
            'div.acordao',
            'div.resultado',
            'tr.resultado',
            'div.jurisprudencia'
        ]
        
        results_found = False
        
        for selector in result_selectors:
            results = soup.select(selector)
            if results:
                results_found = True
                
                for result in results[:5]:  # Limitar a 5 resultados por termo
                    try:
                        # Extrair dados do precedente
                        precedent_data = self._extract_precedent_data(result, search_term)
                        if precedent_data:
                            precedents.append(precedent_data)
                    
                    except Exception as e:
                        logger.warning(f"âš ï¸  Erro ao processar resultado: {str(e)}")
                        continue
                
                break
        
        if not results_found:
            logger.warning(f"âš ï¸  Nenhum resultado estruturado encontrado para '{search_term}'")
            
            # Fallback: criar entrada baseada no termo de busca
            fallback_precedent = {
                "title": f"Precedente STJ sobre {search_term}",
                "content": f"JurisprudÃªncia do Superior Tribunal de JustiÃ§a relacionada a {search_term}. Consultar diretamente no portal do STJ para casos especÃ­ficos.",
                "document_type": "jurisprudencia",
                "category": self._categorize_search_term(search_term),
                "source": "STJ",
                "source_url": f"https://scon.stj.jus.br/SCON/",
                "reference_number": f"STJ-{search_term.replace(' ', '-')}",
                "authority_level": "high",
                "legal_area": ["jurisprudencia"],
                "keywords": search_term.split(),
                "search_term": search_term,
                "collection_date": datetime.now().isoformat()
            }
            precedents.append(fallback_precedent)
        
        return precedents
    
    def _extract_precedent_data(self, result_element, search_term: str) -> Optional[Dict]:
        """Extrai dados de um precedente especÃ­fico"""
        
        try:
            # Procurar por tÃ­tulo/ementa
            title_selectors = ['h3', 'h4', '.titulo', '.ementa', 'strong']
            title = "Precedente STJ"
            
            for selector in title_selectors:
                title_element = result_element.select_one(selector)
                if title_element:
                    title = title_element.get_text(strip=True)[:100]
                    break
            
            # Procurar por conteÃºdo/ementa
            content = result_element.get_text(strip=True)
            
            # Procurar por nÃºmero do processo
            process_pattern = r"(\\d{7}-\\d{2}\\.\\d{4}\\.\\d\\.\\d{2}\\.\\d{4})"
            process_match = re.search(process_pattern, content)
            process_number = process_match.group(1) if process_match else None
            
            return {
                "title": title,
                "content": content[:2000],  # Limitar tamanho
                "document_type": "jurisprudencia",
                "category": self._categorize_search_term(search_term),
                "source": "STJ",
                "source_url": "https://scon.stj.jus.br/SCON/",
                "reference_number": process_number or f"STJ-{search_term.replace(' ', '-')}",
                "authority_level": "high",
                "legal_area": ["jurisprudencia"],
                "keywords": search_term.split() + ["STJ", "precedente"],
                "search_term": search_term,
                "collection_date": datetime.now().isoformat()
            }
            
        except Exception as e:
            logger.warning(f"âš ï¸  Erro ao extrair dados do precedente: {str(e)}")
            return None
    
    def _categorize_search_term(self, search_term: str) -> str:
        """Categoriza termo de busca"""
        
        categories_map = {
            "locaÃ§Ã£o": ["locaÃ§Ã£o", "aluguel", "inquilino", "locador"],
            "telecom": ["telecomunicaÃ§Ãµes", "telefone", "internet", "ANATEL"],
            "financeiro": ["financiamento", "banco", "juros", "crÃ©dito"],
        }
        
        term_lower = search_term.lower()
        
        for category, keywords in categories_map.items():
            if any(keyword in term_lower for keyword in keywords):
                return category
        
        return "geral"
    
    async def save_collected_data(self, data: List[Dict], filename: str = None) -> str:
        """Salva dados coletados em arquivo JSON"""
        
        if not filename:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"legal_data_{timestamp}.json"
        
        filepath = f"collected_data/{filename}"
        
        # Criar diretÃ³rio se nÃ£o existir
        import os
        os.makedirs("collected_data", exist_ok=True)
        
        # Salvar dados
        async with aiofiles.open(filepath, 'w', encoding='utf-8') as f:
            await f.write(json.dumps(data, indent=2, ensure_ascii=False))
        
        logger.info(f"ğŸ’¾ Dados salvos em: {filepath}")
        logger.info(f"ğŸ“Š Total de documentos: {len(data)}")
        
        return filepath

# FunÃ§Ã£o principal para executar coleta
async def run_legal_data_collection():
    """Executa coleta completa de dados jurÃ­dicos"""
    
    logger.info("ğŸš€ Iniciando coleta de dados jurÃ­dicos brasileiros...")
    
    async with LegalDataCollector() as collector:
        
        all_collected_data = []
        
        # 1. Coletar leis do Planalto
        logger.info("\\n" + "="*50)
        logger.info("ğŸ“š COLETANDO LEIS FEDERAIS")
        logger.info("="*50)
        
        laws_data = await collector.collect_planalto_laws()
        all_collected_data.extend(laws_data)
        
        # 2. Coletar jurisprudÃªncia STJ
        logger.info("\\n" + "="*50)
        logger.info("âš–ï¸ COLETANDO JURISPRUDÃŠNCIA STJ")
        logger.info("="*50)
        
        search_terms = [
            "clÃ¡usula abusiva contrato",
            "locaÃ§Ã£o inquilino multa",
            "telecomunicaÃ§Ãµes cancelamento serviÃ§o",
            "financiamento taxa juros abusiva",
            "consumidor fornecedor direitos"
        ]
        
        jurisprudence_data = await collector.collect_stj_jurisprudence(search_terms)
        all_collected_data.extend(jurisprudence_data)
        
        # 3. Salvar dados coletados
        logger.info("\\n" + "="*50)
        logger.info("ğŸ’¾ SALVANDO DADOS COLETADOS")
        logger.info("="*50)
        
        saved_file = await collector.save_collected_data(all_collected_data)
        
        # 4. EstatÃ­sticas finais
        logger.info("\\n" + "="*50)
        logger.info("ğŸ“Š ESTATÃSTICAS FINAIS")
        logger.info("="*50)
        
        stats = {
            "total_documentos": len(all_collected_data),
            "leis_coletadas": len([d for d in all_collected_data if d["document_type"] == "lei"]),
            "jurisprudencia_coletada": len([d for d in all_collected_data if d["document_type"] == "jurisprudencia"]),
            "categorias": {}
        }
        
        for doc in all_collected_data:
            category = doc.get("category", "geral")
            stats["categorias"][category] = stats["categorias"].get(category, 0) + 1
        
        logger.info(f"ğŸ“‹ Total de documentos coletados: {stats['total_documentos']}")
        logger.info(f"ğŸ“œ Leis federais: {stats['leis_coletadas']}")
        logger.info(f"âš–ï¸ Precedentes jurisprudenciais: {stats['jurisprudencia_coletada']}")
        logger.info(f"ğŸ“ Categorias: {stats['categorias']}")
        logger.info(f"ğŸ’¾ Arquivo salvo: {saved_file}")
        
        logger.info("\\nğŸ‰ Coleta de dados jurÃ­dicos concluÃ­da com sucesso!")
        
        return all_collected_data, saved_file

if __name__ == "__main__":
    collected_data, file_path = asyncio.run(run_legal_data_collection())