"""
AI Bias Audit Framework - Democratiza AI
Sistema de detec√ß√£o e mitiga√ß√£o de vieses algor√≠tmicos
"""

from dataclasses import dataclass, field
from typing import Dict, List, Optional, Any, Set, Tuple
from enum import Enum
from datetime import datetime, timedelta
import json
import statistics
from collections import defaultdict, Counter
import re

class BiasType(Enum):
    """Tipos de vi√©s identific√°veis"""
    GENDER = "genero"  # Vi√©s de g√™nero
    SOCIOECONOMIC = "socioeconomico"  # Vi√©s socioecon√¥mico
    GEOGRAPHIC = "geografico"  # Vi√©s regional/geogr√°fico
    AGE = "idade"  # Vi√©s et√°rio
    ENTITY_SIZE = "tamanho_entidade"  # Vi√©s contra PF vs PJ
    CONTRACT_TYPE = "tipo_contrato"  # Vi√©s por tipo de contrato
    LANGUAGE = "linguagem"  # Vi√©s lingu√≠stico/t√©cnico
    RISK_ASSESSMENT = "avaliacao_risco"  # Vi√©s na avalia√ß√£o de riscos

class BiasCategory(Enum):
    """Categorias de severidade do vi√©s"""
    CRITICAL = "critico"  # Impacto direto em direitos fundamentais
    HIGH = "alto"  # Impacto significativo na an√°lise
    MEDIUM = "medio"  # Impacto moderado
    LOW = "baixo"  # Impacto m√≠nimo detect√°vel

@dataclass
class BiasIndicator:
    """Indicador espec√≠fico de vi√©s"""
    bias_type: BiasType
    category: BiasCategory
    description: str
    detection_pattern: str  # Regex ou descri√ß√£o do padr√£o
    mitigation_strategy: str
    examples: List[str] = field(default_factory=list)
    
@dataclass
class BiasAuditResult:
    """Resultado de auditoria de vi√©s"""
    audit_id: str
    timestamp: datetime
    contract_type: str
    entity_type: str  # CPF/CNPJ
    detected_biases: List[BiasIndicator]
    confidence_scores: Dict[str, float]
    recommendation: str
    requires_human_review: bool = False

class AIBiasAuditor:
    """
    Auditor de vieses em an√°lises contratuais de IA
    
    Detecta e mitiga:
    1. Vieses discriminat√≥rios em an√°lises
    2. Disparidades de tratamento por tipo de entidade
    3. Linguagem tendenciosa ou excludente  
    4. Avalia√ß√µes de risco enviesadas
    5. Recomenda√ß√µes desproporcionais
    """
    
    def __init__(self):
        self.bias_indicators = self._initialize_bias_indicators()
        self.audit_history: List[BiasAuditResult] = []
        self.bias_patterns = self._compile_bias_patterns()
        
    def _initialize_bias_indicators(self) -> List[BiasIndicator]:
        """Inicializa indicadores de vi√©s conhecidos"""
        indicators = []
        
        # Vi√©s de g√™nero
        indicators.append(BiasIndicator(
            bias_type=BiasType.GENDER,
            category=BiasCategory.HIGH,
            description="Linguagem que assume g√™nero ou usa estere√≥tipos de g√™nero",
            detection_pattern=r"\b(homem|mulher|masculino|feminino)\b.*\b(deve|deveria|precisa|respons√°vel)\b",
            mitigation_strategy="Usar linguagem neutra e inclusiva. Evitar assumir responsabilidades baseadas em g√™nero.",
            examples=[
                "A mulher deve ter cuidado especial com...",
                "O homem √© respons√°vel por...",
                "T√≠pico de contratos femininos..."
            ]
        ))
        
        # Vi√©s socioecon√¥mico
        indicators.append(BiasIndicator(
            bias_type=BiasType.SOCIOECONOMIC,
            category=BiasCategory.CRITICAL,
            description="Discrimina√ß√£o baseada em classe social ou poder econ√¥mico",
            detection_pattern=r"\b(pobre|rico|classe baixa|elite|privilegiado|carente)\b",
            mitigation_strategy="Focar em an√°lise t√©cnica objetiva. Evitar julgamentos socioecon√¥micos.",
            examples=[
                "Pessoas de baixa renda n√£o compreendem...",
                "Contrato t√≠pico de classe alta...",
                "Inadequado para pessoas simples..."
            ]
        ))
        
        # Vi√©s geogr√°fico
        indicators.append(BiasIndicator(
            bias_type=BiasType.GEOGRAPHIC,
            category=BiasCategory.MEDIUM,
            description="Estere√≥tipos ou discrimina√ß√£o regional",
            detection_pattern=r"\b(interior|capital|nordeste|sul|sudeste)\b.*\b(costuma|t√≠pico|comum|normal)\b",
            mitigation_strategy="Aplicar an√°lise uniforme independente da regi√£o. Considerar apenas aspectos legais locais.",
            examples=[
                "No interior √© comum esse tipo de cl√°usula...",
                "Contratos do Nordeste geralmente...",
                "Mais sofisticado que o normal para essa regi√£o..."
            ]
        ))
        
        # Vi√©s contra pessoas f√≠sicas
        indicators.append(BiasIndicator(
            bias_type=BiasType.ENTITY_SIZE,
            category=BiasCategory.HIGH,
            description="Tratamento diferenciado injustificado entre PF e PJ",
            detection_pattern=r"\b(apenas|s√≥|somente)\b.*\b(pessoa f√≠sica|CPF)\b.*\b(n√£o|nunca|jamais)\b",
            mitigation_strategy="Aplicar prote√ß√µes adequadas por tipo de entidade conforme lei, sem discrimina√ß√£o adicional.",
            examples=[
                "Pessoa f√≠sica nunca deveria aceitar...",
                "S√≥ empresas conseguem negociar...",
                "CPF sempre sai perdendo..."
            ]
        ))
        
        # Vi√©s na avalia√ß√£o de risco
        indicators.append(BiasIndicator(
            bias_type=BiasType.RISK_ASSESSMENT,
            category=BiasCategory.CRITICAL,
            description="Avalia√ß√£o de risco desproporcional ou enviesada",
            detection_pattern=r"\b(extremamente perigoso|nunca assine|fuja|golpe|armadilha)\b",
            mitigation_strategy="Usar escala proporcional de risco. Explicar riscos objetivamente sem alarmismo.",
            examples=[
                "Extremamente perigoso para qualquer pessoa...",
                "Nunca assine este tipo de contrato...",
                "√â uma armadilha √≥bvia..."
            ]
        ))
        
        # Vi√©s lingu√≠stico/t√©cnico
        indicators.append(BiasIndicator(
            bias_type=BiasType.LANGUAGE,
            category=BiasCategory.MEDIUM,
            description="Linguagem excessivamente t√©cnica ou excludente",
            detection_pattern=r"\b(obviamente|evidentemente|qualquer pessoa sabe|√© √≥bvio)\b",
            mitigation_strategy="Explicar conceitos de forma acess√≠vel. N√£o assumir conhecimento pr√©vio.",
            examples=[
                "Obviamente voc√™ deveria saber que...",
                "√â evidente para qualquer pessoa...",
                "Qualquer leigo compreende..."
            ]
        ))
        
        return indicators
    
    def _compile_bias_patterns(self) -> Dict[BiasType, List[str]]:
        """Compila padr√µes de detec√ß√£o por tipo de vi√©s"""
        patterns = defaultdict(list)
        
        for indicator in self.bias_indicators:
            patterns[indicator.bias_type].append(indicator.detection_pattern)
        
        return dict(patterns)
    
    def audit_response(self, 
                      response_text: str,
                      contract_type: str = "",
                      entity_type: str = "",
                      context: Dict[str, Any] = None) -> BiasAuditResult:
        """Executa auditoria completa de vi√©s na resposta"""
        
        audit_id = f"audit_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        detected_biases = []
        confidence_scores = {}
        
        # Detecta cada tipo de vi√©s
        for indicator in self.bias_indicators:
            score = self._detect_bias_indicator(response_text, indicator)
            confidence_scores[indicator.bias_type.value] = score
            
            if score > 0.5:  # Threshold para detec√ß√£o
                detected_biases.append(indicator)
        
        # An√°lises espec√≠ficas adicionais
        detected_biases.extend(self._detect_contextual_biases(
            response_text, contract_type, entity_type, context or {}
        ))
        
        # Determina se requer revis√£o humana
        requires_review = any(
            bias.category in [BiasCategory.CRITICAL, BiasCategory.HIGH] 
            for bias in detected_biases
        )
        
        # Gera recomenda√ß√£o
        recommendation = self._generate_bias_mitigation_recommendation(detected_biases)
        
        result = BiasAuditResult(
            audit_id=audit_id,
            timestamp=datetime.now(),
            contract_type=contract_type,
            entity_type=entity_type,
            detected_biases=detected_biases,
            confidence_scores=confidence_scores,
            recommendation=recommendation,
            requires_human_review=requires_review
        )
        
        self.audit_history.append(result)
        return result
    
    def _detect_bias_indicator(self, text: str, indicator: BiasIndicator) -> float:
        """Detecta indicador espec√≠fico de vi√©s no texto"""
        
        # Detec√ß√£o por padr√£o regex
        matches = re.findall(indicator.detection_pattern, text.lower(), re.IGNORECASE)
        
        if not matches:
            return 0.0
        
        # Calcula confian√ßa baseada no n√∫mero de matches
        confidence = min(len(matches) / 3.0, 1.0)  # M√°ximo 1.0 com 3+ matches
        
        # Ajusta confian√ßa baseada na categoria
        if indicator.category == BiasCategory.CRITICAL:
            confidence *= 1.2  # Aumenta sensibilidade para vieses cr√≠ticos
        elif indicator.category == BiasCategory.LOW:
            confidence *= 0.8  # Diminui sensibilidade para vieses baixos
        
        return min(confidence, 1.0)
    
    def _detect_contextual_biases(self, 
                                text: str, 
                                contract_type: str,
                                entity_type: str,
                                context: Dict[str, Any]) -> List[BiasIndicator]:
        """Detecta vieses contextuais espec√≠ficos"""
        
        contextual_biases = []
        
        # Vi√©s de avalia√ß√£o de risco desproporcional
        risk_words = re.findall(r'\b(risco|perigo|cuidado|aten√ß√£o)\b', text.lower())
        negative_words = re.findall(r'\b(nunca|jamais|evite|fuja|perigoso)\b', text.lower())
        
        if len(risk_words) > 3 and len(negative_words) > 2:
            contextual_biases.append(BiasIndicator(
                bias_type=BiasType.RISK_ASSESSMENT,
                category=BiasCategory.HIGH,
                description="Avalia√ß√£o de risco excessivamente alarmista",
                detection_pattern="M√∫ltiplas palavras de alarme concentradas",
                mitigation_strategy="Balancear avalia√ß√£o de risco com linguagem proporcional e construtiva."
            ))
        
        # Vi√©s de complexidade vs tipo de entidade
        if entity_type == "CPF":
            complex_terms = re.findall(r'\b(ipso facto|ad hoc|stricto sensu|lato sensu)\b', text)
            if len(complex_terms) > 2:
                contextual_biases.append(BiasIndicator(
                    bias_type=BiasType.LANGUAGE,
                    category=BiasCategory.MEDIUM,
                    description="Linguagem excessivamente t√©cnica para pessoa f√≠sica",
                    detection_pattern="Termos jur√≠dicos latinos em excesso",
                    mitigation_strategy="Simplificar linguagem para maior acessibilidade."
                ))
        
        return contextual_biases
    
    def _generate_bias_mitigation_recommendation(self, 
                                               detected_biases: List[BiasIndicator]) -> str:
        """Gera recomenda√ß√£o para mitigar vieses detectados"""
        
        if not detected_biases:
            return "‚úÖ Nenhum vi√©s significativo detectado. Resposta aprovada para uso."
        
        recommendations = []
        
        # Agrupa por categoria de severidade
        critical_biases = [b for b in detected_biases if b.category == BiasCategory.CRITICAL]
        high_biases = [b for b in detected_biases if b.category == BiasCategory.HIGH]
        
        if critical_biases:
            recommendations.append("üö® A√á√ÉO IMEDIATA NECESS√ÅRIA:")
            for bias in critical_biases:
                recommendations.append(f"   ‚Ä¢ {bias.mitigation_strategy}")
        
        if high_biases:
            recommendations.append("\n‚ö†Ô∏è  REVIS√ÉO RECOMENDADA:")
            for bias in high_biases:
                recommendations.append(f"   ‚Ä¢ {bias.mitigation_strategy}")
        
        # Recomenda√ß√£o geral
        recommendations.append(
            "\nüìã DIRETRIZES GERAIS:\n"
            "   ‚Ä¢ Use linguagem inclusiva e neutra\n"
            "   ‚Ä¢ Foque na an√°lise t√©cnica objetiva\n"
            "   ‚Ä¢ Evite generaliza√ß√µes e estere√≥tipos\n"
            "   ‚Ä¢ Mantenha tom educativo e respeitoso"
        )
        
        return "\n".join(recommendations)
    
    def get_bias_statistics(self, days: int = 30) -> Dict[str, Any]:
        """Gera estat√≠sticas de vi√©s dos √∫ltimos dias"""
        
        cutoff_date = datetime.now() - timedelta(days=days)
        recent_audits = [
            audit for audit in self.audit_history 
            if audit.timestamp >= cutoff_date
        ]
        
        if not recent_audits:
            return {"message": "Nenhuma auditoria no per√≠odo especificado"}
        
        # Estat√≠sticas por tipo de vi√©s
        bias_counts = Counter()
        for audit in recent_audits:
            for bias in audit.detected_biases:
                bias_counts[bias.bias_type.value] += 1
        
        # Taxa de revis√£o humana necess√°ria
        human_review_rate = sum(
            1 for audit in recent_audits if audit.requires_human_review
        ) / len(recent_audits)
        
        # Vieses mais comuns
        most_common_biases = bias_counts.most_common(5)
        
        return {
            "period_days": days,
            "total_audits": len(recent_audits),
            "human_review_rate": round(human_review_rate * 100, 2),
            "bias_detection_rate": round(
                sum(1 for audit in recent_audits if audit.detected_biases) / len(recent_audits) * 100, 2
            ),
            "most_common_biases": [
                {"type": bias_type, "count": count, "percentage": round(count/len(recent_audits)*100, 2)}
                for bias_type, count in most_common_biases
            ],
            "bias_by_entity_type": self._analyze_bias_by_entity_type(recent_audits),
            "recommendations": self._generate_system_recommendations(recent_audits)
        }
    
    def _analyze_bias_by_entity_type(self, audits: List[BiasAuditResult]) -> Dict[str, Any]:
        """Analisa vieses por tipo de entidade"""
        
        cpf_audits = [a for a in audits if a.entity_type == "CPF"]
        cnpj_audits = [a for a in audits if a.entity_type == "CNPJ"]
        
        cpf_bias_rate = sum(1 for a in cpf_audits if a.detected_biases) / len(cpf_audits) if cpf_audits else 0
        cnpj_bias_rate = sum(1 for a in cnpj_audits if a.detected_biases) / len(cnpj_audits) if cnpj_audits else 0
        
        return {
            "cpf_audits": len(cpf_audits),
            "cnpj_audits": len(cnpj_audits),
            "cpf_bias_rate": round(cpf_bias_rate * 100, 2),
            "cnpj_bias_rate": round(cnpj_bias_rate * 100, 2),
            "disparity": abs(cpf_bias_rate - cnpj_bias_rate) > 0.1  # >10% diferen√ßa indica problema
        }
    
    def _generate_system_recommendations(self, audits: List[BiasAuditResult]) -> List[str]:
        """Gera recomenda√ß√µes para melhorar o sistema"""
        
        recommendations = []
        
        # Analisa padr√µes nos vieses
        all_biases = [bias for audit in audits for bias in audit.detected_biases]
        bias_types = Counter(bias.bias_type for bias in all_biases)
        
        if bias_types[BiasType.LANGUAGE] > len(audits) * 0.3:
            recommendations.append("Revisar prompts para linguagem mais acess√≠vel")
        
        if bias_types[BiasType.RISK_ASSESSMENT] > len(audits) * 0.2:
            recommendations.append("Calibrar sistema de avalia√ß√£o de riscos")
        
        if bias_types[BiasType.GENDER] > 0:
            recommendations.append("Implementar filtros de linguagem inclusiva")
        
        high_review_rate = sum(1 for audit in audits if audit.requires_human_review) / len(audits)
        if high_review_rate > 0.4:
            recommendations.append("Melhorar treinamento do modelo base")
        
        return recommendations
    
    def export_audit_report(self, days: int = 30) -> Dict[str, Any]:
        """Exporta relat√≥rio completo de auditoria"""
        
        stats = self.get_bias_statistics(days)
        
        return {
            "report_type": "AI Bias Audit Report",
            "generated_at": datetime.now().isoformat(),
            "statistics": stats,
            "bias_indicators_count": len(self.bias_indicators),
            "audit_methodology": {
                "detection_methods": [
                    "Pattern matching com regex",
                    "An√°lise contextual",
                    "Scoring de confian√ßa",
                    "Categoriza√ß√£o por severidade"
                ],
                "bias_types_monitored": [bt.value for bt in BiasType],
                "threshold_detection": 0.5,
                "human_review_triggers": ["CRITICAL", "HIGH"]
            },
            "compliance_notes": [
                "Auditoria cont√≠nua de vieses algor√≠tmicos",
                "Monitoramento de equidade por tipo de entidade",
                "Detec√ß√£o de discrimina√ß√£o em an√°lises jur√≠dicas",
                "Conformidade com princ√≠pios de IA √©tica"
            ]
        }

# Inst√¢ncia global
bias_auditor = AIBiasAuditor()