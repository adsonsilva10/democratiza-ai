# Democratiza AI - Configuração de LLMs para Roteamento Inteligente

# =============================================================================
# ANTHROPIC CLAUDE (Obrigatório - Modelo Principal)
# =============================================================================
# Obter em: https://console.anthropic.com/
# Usado para: Análises médias, complexas e especializadas
ANTHROPIC_API_KEY=sk-ant-api03-XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX

# =============================================================================
# GROQ LLAMA (Opcional - Modelo Econômico)
# =============================================================================
# Obter em: https://console.groq.com/
# Usado para: Análises simples e triagem rápida
GROQ_API_KEY=gsk_XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX

# =============================================================================
# OPENAI GPT (Futuro - Implementação Planejada)
# =============================================================================
# Para futuras integrações
# OPENAI_API_KEY=sk-XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX

# =============================================================================
# CONFIGURAÇÕES DO ROTEADOR DE LLM
# =============================================================================

# Threshold de complexidade para roteamento automático
LLM_COMPLEXITY_THRESHOLD_SIMPLE=5      # Abaixo disso: Groq Llama
LLM_COMPLEXITY_THRESHOLD_MEDIUM=12     # 5-12: Anthropic Haiku  
LLM_COMPLEXITY_THRESHOLD_COMPLEX=20    # 12-20: Anthropic Sonnet
# Acima de 20: Anthropic Opus

# Custos máximos por análise (em USD)
LLM_MAX_COST_PER_ANALYSIS=0.50        # Limite máximo por análise
LLM_MONTHLY_BUDGET=500.00             # Budget mensal total

# Configurações de performance
LLM_MAX_RESPONSE_TIME=30               # Timeout em segundos
LLM_ENABLE_STREAMING=true              # Habilitar streaming de respostas
LLM_CACHE_RESPONSES=true               # Cache de respostas similares

# Configurações de qualidade
LLM_MIN_CONFIDENCE_SCORE=0.7           # Score mínimo de confiança
LLM_ENABLE_FALLBACK=true              # Fallback para modelo superior se necessário
LLM_QUALITY_THRESHOLD=0.8             # Threshold para upgrade automático

# =============================================================================
# MÉTRICAS E MONITORAMENTO
# =============================================================================

# Habilitar coleta de métricas detalhadas
LLM_ENABLE_METRICS=true
LLM_METRICS_INTERVAL=3600             # Intervalo de coleta em segundos (1h)
LLM_COST_ALERTS=true                  # Alertas de custo
LLM_PERFORMANCE_ALERTS=true           # Alertas de performance

# Limites para alertas
LLM_DAILY_COST_ALERT=50.00           # Alerta diário de custo
LLM_HOURLY_REQUEST_LIMIT=100         # Limite de requests por hora

# =============================================================================
# CONFIGURAÇÕES ESPECÍFICAS POR MODELO
# =============================================================================

# Groq Llama
GROQ_MODEL_NAME=llama-3.1-70b-versatile
GROQ_MAX_TOKENS=8192
GROQ_TEMPERATURE=0.3
GROQ_TIMEOUT=15

# Anthropic Haiku  
ANTHROPIC_HAIKU_MODEL=claude-3-haiku-20240307
ANTHROPIC_HAIKU_MAX_TOKENS=4096
ANTHROPIC_HAIKU_TEMPERATURE=0.2

# Anthropic Sonnet
ANTHROPIC_SONNET_MODEL=claude-3-5-sonnet-20240620  
ANTHROPIC_SONNET_MAX_TOKENS=8192
ANTHROPIC_SONNET_TEMPERATURE=0.1

# Anthropic Opus
ANTHROPIC_OPUS_MODEL=claude-3-opus-20240229
ANTHROPIC_OPUS_MAX_TOKENS=4096  
ANTHROPIC_OPUS_TEMPERATURE=0.0

# =============================================================================
# CONFIGURAÇÕES RAG (Retrieval Augmented Generation)
# =============================================================================

# Integração com base de conhecimento jurídico
RAG_ENABLE=true                       # Habilitar RAG
RAG_MAX_DOCUMENTS=5                   # Máximo de documentos por consulta
RAG_SIMILARITY_THRESHOLD=0.7          # Threshold de similaridade
RAG_EMBEDDING_MODEL=text-embedding-3-small  # Modelo de embeddings

# =============================================================================
# CONFIGURAÇÕES DE DESENVOLVIMENTO
# =============================================================================

# Debug e logs detalhados
LLM_DEBUG=false                       # Debug mode
LLM_LOG_LEVEL=INFO                    # DEBUG, INFO, WARNING, ERROR
LLM_LOG_REQUESTS=true                 # Log todas as requisições
LLM_LOG_RESPONSES=false               # Log respostas (cuidado com dados sensíveis)

# Modo de teste
LLM_TEST_MODE=false                   # Usar mocks em vez de APIs reais
LLM_MOCK_RESPONSES=false              # Respostas simuladas para testes